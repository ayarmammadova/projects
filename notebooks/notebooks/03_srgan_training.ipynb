{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e8a521",
   "metadata": {},
   "source": [
    "# SRGAN Training on CelebA / DIV2K (PixelForge)\n",
    "This notebook trains a Super-Resolution GAN (SRGAN) using the preprocessed LR/HR pairs.\n",
    "It assumes the dataset has been prepared in `data/celeba/...` or `data/div2k/...` and split into train/val.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8ad6c",
   "metadata": {},
   "source": [
    "1. 环境 & 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119d47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/notebooks\n",
      "Base dir (project root): /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project\n",
      "Tensor data dir: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/notebooks/outputs/final_tensors_sharded\n",
      "Tensor dir exists: True\n",
      "Src dir exists: True\n",
      "Exp dir: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/experiments/celeba_srgan_run1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 本地项目根目录 - 更可靠的路径检测\n",
    "# 方法：从当前工作目录向上查找，直到找到包含 src 目录的目录\n",
    "current_dir = Path(\".\").resolve()\n",
    "BASE_DIR = current_dir\n",
    "\n",
    "# 向上查找项目根目录（包含 src 的目录）\n",
    "max_levels = 3  # 最多向上查找3层\n",
    "for level in range(max_levels + 1):\n",
    "    test_dir = current_dir\n",
    "    for _ in range(level):\n",
    "        test_dir = test_dir.parent\n",
    "    if (test_dir / \"src\").exists() and (test_dir / \"src\" / \"CelebASRDataset.py\").exists():\n",
    "        BASE_DIR = test_dir\n",
    "        break\n",
    "\n",
    "# 数据目录可能在多个位置，按优先级查找\n",
    "# 1. notebooks/outputs/final_tensors_sharded (如果从 notebooks 运行)\n",
    "# 2. data/celeba/final_tensors_sharded_fast (项目根目录的标准位置)\n",
    "notebooks_data_dir = BASE_DIR / \"notebooks\" / \"outputs\" / \"final_tensors_sharded\"\n",
    "standard_data_dir = BASE_DIR / \"data\" / \"celeba\" / \"final_tensors_sharded_fast\"\n",
    "\n",
    "if notebooks_data_dir.exists() and (notebooks_data_dir / \"manifest_train.json\").exists():\n",
    "    DATA_DIR = notebooks_data_dir.parent.parent  # 指向 notebooks 目录的父目录，以便 DATA_DIR / \"celeba\" 能工作\n",
    "    TENSOR_DIR = notebooks_data_dir\n",
    "elif standard_data_dir.exists():\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    TENSOR_DIR = standard_data_dir\n",
    "else:\n",
    "    # 默认使用 notebooks/outputs\n",
    "    DATA_DIR = BASE_DIR / \"notebooks\"\n",
    "    TENSOR_DIR = BASE_DIR / \"notebooks\" / \"outputs\" / \"final_tensors_sharded\"\n",
    "\n",
    "EXP_DIR = BASE_DIR / \"experiments\" / \"celeba_srgan_run1\"\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Current working dir:\", Path(\".\").resolve())\n",
    "print(\"Base dir (project root):\", BASE_DIR)\n",
    "print(\"Tensor data dir:\", TENSOR_DIR)\n",
    "print(\"Tensor dir exists:\", TENSOR_DIR.exists())\n",
    "print(\"Src dir exists:\", (BASE_DIR / \"src\").exists())\n",
    "print(\"Exp dir:\", EXP_DIR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807ae84",
   "metadata": {},
   "source": [
    "2. Dataset 加载（用前面写过的 src/）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee06f281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/src\n",
      "Src dir exists: True\n",
      "Cleared CelebASRDataset from sys.modules\n",
      "✓ Successfully imported CelebASRDataset\n",
      "\n",
      "Using tensor data dir: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/notebooks/outputs/final_tensors_sharded\n",
      "Tensor dir exists: True\n",
      "\n",
      "✓ Datasets loaded successfully!\n",
      "Train batches: 11397\n",
      "Val batches: 634\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "# 添加 src 目录到 Python 路径\n",
    "src_path = BASE_DIR / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(f\"Added to sys.path: {src_path}\")\n",
    "print(f\"Src dir exists: {src_path.exists()}\")\n",
    "\n",
    "# 清除可能存在的模块缓存\n",
    "if 'CelebASRDataset' in sys.modules:\n",
    "    del sys.modules['CelebASRDataset']\n",
    "    print(\"Cleared CelebASRDataset from sys.modules\")\n",
    "\n",
    "# 清除 __pycache__ 以确保使用最新代码\n",
    "import shutil\n",
    "pycache_path = src_path / \"__pycache__\"\n",
    "if pycache_path.exists():\n",
    "    shutil.rmtree(pycache_path)\n",
    "    print(\"Cleared __pycache__\")\n",
    "\n",
    "# 导入数据集类\n",
    "try:\n",
    "    from CelebASRDataset import CelebASRDataset\n",
    "    print(\"✓ Successfully imported CelebASRDataset\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(f\"Trying to inspect the module...\")\n",
    "    import os\n",
    "    celeb_file = src_path / \"CelebASRDataset.py\"\n",
    "    if celeb_file.exists():\n",
    "        print(f\"File exists: {celeb_file}\")\n",
    "        with open(celeb_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            if 'class CelebASRDataset' in content:\n",
    "                print(\"✓ Class definition found in file\")\n",
    "            else:\n",
    "                print(\"✗ Class definition NOT found in file\")\n",
    "    raise\n",
    "\n",
    "# 如果你文件名不完全一致，改这里\n",
    "# from DIV2KSRDataset import DIV2KSRDataset\n",
    "\n",
    "# 使用检测到的数据目录\n",
    "print(f\"\\nUsing tensor data dir: {TENSOR_DIR}\")\n",
    "print(f\"Tensor dir exists: {TENSOR_DIR.exists()}\")\n",
    "\n",
    "# 这里选一个数据集\n",
    "train_dataset = CelebASRDataset(\n",
    "    root_dir=TENSOR_DIR,\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "val_dataset = CelebASRDataset(\n",
    "    root_dir=TENSOR_DIR,\n",
    "    split=\"val\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"\\n✓ Datasets loaded successfully!\")\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73687c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Path Verification:\n",
      "============================================================\n",
      "BASE_DIR: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project\n",
      "BASE_DIR exists: True\n",
      "\n",
      "Src directory: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/src\n",
      "Src exists: True\n",
      "Files in src: ['transforms.py', '.DS_Store', 'CelebASRDataset.py', '__pycache__', 'dataset.py', 'DIV2KSRDataset.py']\n",
      "\n",
      "Tensor data directory: /Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/notebooks/outputs/final_tensors_sharded\n",
      "Tensor dir exists: True\n",
      "Manifest files: ['manifest_test.json', 'manifest_train.json', 'manifest_val.json']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 验证路径设置\n",
    "import os\n",
    "print(\"=\" * 60)\n",
    "print(\"Path Verification:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"BASE_DIR exists: {BASE_DIR.exists()}\")\n",
    "print(f\"\\nSrc directory: {BASE_DIR / 'src'}\")\n",
    "print(f\"Src exists: {(BASE_DIR / 'src').exists()}\")\n",
    "if (BASE_DIR / \"src\").exists():\n",
    "    print(f\"Files in src: {os.listdir(BASE_DIR / 'src')}\")\n",
    "print(f\"\\nTensor data directory: {TENSOR_DIR}\")\n",
    "print(f\"Tensor dir exists: {TENSOR_DIR.exists()}\")\n",
    "if TENSOR_DIR.exists():\n",
    "    manifest_files = [f for f in os.listdir(TENSOR_DIR) if f.startswith(\"manifest_\")]\n",
    "    print(f\"Manifest files: {manifest_files}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c2385",
   "metadata": {},
   "source": [
    "3. 模型占位（Generator / Discriminator）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6652aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G params: 0.017348 M\n",
      "D params: 0.018177 M\n"
     ]
    }
   ],
   "source": [
    "# ====== Generator (placeholder) ======\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: replace with Yosr's SRGAN generator\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 9, padding=4),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 3, 3, padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ====== Discriminator (placeholder) ======\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hr_size=128):\n",
    "        super().__init__()\n",
    "        # TODO: replace with Yosr's SRGAN discriminator\n",
    "        # 计算经过卷积后的特征图尺寸\n",
    "        # 输入: (batch, 3, hr_size, hr_size)\n",
    "        # Conv1: stride=2, padding=1, kernel=3 -> (batch, 64, hr_size//2, hr_size//2)\n",
    "        # 对于 hr_size=128: 输出是 (batch, 64, 64, 64)\n",
    "        # 使用 AdaptiveAvgPool2d 来避免硬编码尺寸\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),  # (128,128) -> (64,64)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # (64,64) -> (32,32)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),  # (32,32) -> (16,16)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),  # (16,16) -> (8,8)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),  # (8,8) -> (1,1)\n",
    "            nn.Flatten(),  # (batch, 512)\n",
    "            nn.Linear(512, 1)  # (batch, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "generator     = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "print(\"G params:\", sum(p.numel() for p in generator.parameters())/1e6, \"M\")\n",
    "print(\"D params:\", sum(p.numel() for p in discriminator.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce935d06",
   "metadata": {},
   "source": [
    "4. 损失函数 & 优化器\n",
    "SRGAN 标配是 content loss (VGG) + adversarial loss。现在先放一个简化版，能跑就行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7cf0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss   = nn.BCEWithLogitsLoss().to(device)\n",
    "l1_loss    = nn.L1Loss().to(device)\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. 训练循环（简化版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51b19f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liangwenlong/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x262144 and 16384x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 1) Train Discriminator\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[32m     20\u001b[39m sr = generator(lr).detach()  \u001b[38;5;66;03m# 生成的假图\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m real_logits = \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m fake_logits = discriminator(sr)\n\u001b[32m     24\u001b[39m real_labels = torch.ones_like(real_logits)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mDiscriminator.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/bme/3/DeepLearning/Topic/SGN/SuperResolution_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (16x262144 and 16384x1)"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "num_epochs = 5\n",
    "sample_every = 200  # 每多少 step 存一张图\n",
    "\n",
    "history = defaultdict(list)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # 你的 Dataset 应该返回 (lr, hr)\n",
    "        lr, hr = batch\n",
    "        lr = lr.to(device)\n",
    "        hr = hr.to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        # 1) Train Discriminator\n",
    "        # ---------------------\n",
    "        sr = generator(lr).detach()  # 生成的假图\n",
    "        real_logits = discriminator(hr)\n",
    "        fake_logits = discriminator(sr)\n",
    "\n",
    "        real_labels = torch.ones_like(real_logits)\n",
    "        fake_labels = torch.zeros_like(fake_logits)\n",
    "\n",
    "        d_loss_real = bce_loss(real_logits, real_labels)\n",
    "        d_loss_fake = bce_loss(fake_logits, fake_labels)\n",
    "        d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # 2) Train Generator\n",
    "        # ---------------------\n",
    "        sr = generator(lr)\n",
    "        fake_logits = discriminator(sr)\n",
    "        adv_loss = bce_loss(fake_logits, torch.ones_like(fake_logits))\n",
    "        content_loss = l1_loss(sr, hr)\n",
    "        g_loss = content_loss + 1e-3 * adv_loss\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # log\n",
    "        history[\"d_loss\"].append(d_loss.item())\n",
    "        history[\"g_loss\"].append(g_loss.item())\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{step+1}/{len(train_loader)}] \"\n",
    "                  f\"D: {d_loss.item():.4f}  G: {g_loss.item():.4f}\")\n",
    "\n",
    "        # 保存可视化样例\n",
    "        if (step + 1) % sample_every == 0:\n",
    "            out_dir = EXP_DIR / \"samples\"\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            # 反归一化要看你前面 transforms 怎么做的，这里先直接存\n",
    "            grid = sr[0].detach().cpu().permute(1,2,0).numpy()\n",
    "            plt.imshow(grid)\n",
    "            plt.title(f\"epoch{epoch+1}_step{step+1}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(out_dir / f\"e{epoch+1}_s{step+1}.png\")\n",
    "            plt.close()\n",
    "\n",
    "    # 每个 epoch 保存一次权重\n",
    "    torch.save(generator.state_dict(), EXP_DIR / f\"generator_epoch{epoch+1}.pth\")\n",
    "    torch.save(discriminator.state_dict(), EXP_DIR / f\"discriminator_epoch{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650e943",
   "metadata": {},
   "source": [
    "6. 验证 / PSNR / SSIM（可选）\n",
    "这里的数据范围我写了 data_range=1.0，如果是 0–255 记得改回 255。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "def evaluate_psnr_ssim(model, dataloader, max_batches=10):\n",
    "    model.eval()\n",
    "    psnrs, ssims = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (lr, hr) in enumerate(dataloader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "            sr = model(lr)\n",
    "\n",
    "            # 转 numpy\n",
    "            sr_np = sr[0].detach().cpu().permute(1,2,0).numpy()\n",
    "            hr_np = hr[0].detach().cpu().permute(1,2,0).numpy()\n",
    "\n",
    "            psnrs.append(peak_signal_noise_ratio(hr_np, sr_np, data_range=1.0))\n",
    "            ssims.append(structural_similarity(hr_np, sr_np, channel_axis=2, data_range=1.0))\n",
    "\n",
    "    return sum(psnrs)/len(psnrs), sum(ssims)/len(ssims)\n",
    "\n",
    "psnr, ssim = evaluate_psnr_ssim(generator, val_loader, max_batches=5)\n",
    "print(\"Val PSNR:\", psnr, \" Val SSIM:\", ssim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b7ba7c",
   "metadata": {},
   "source": [
    "7. 保存训练日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d698216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "log_path = EXP_DIR / \"train_log.json\"\n",
    "with open(log_path, \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "print(\"Saved train log to\", log_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
