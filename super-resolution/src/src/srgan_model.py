# -*- coding: utf-8 -*-
"""SRGAN_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_LAhbfvM3B6dXY8CikiIL14BPrwOj54E
"""

import torch, platform, sys
print("Python:", sys.version.split()[0])
print("PyTorch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

!pip -q install kaggle torch torchvision matplotlib

from pathlib import Path
import os

KAGGLE_TOKEN = Path("/root/.kaggle/kaggle.json")
if not KAGGLE_TOKEN.exists():
    from google.colab import files
    print(" Upload your Kaggle API token (kaggle.json). Get it from Kaggle: Account â†’ Create New API Token.")
    uploaded = files.upload()
    os.makedirs("/root/.kaggle", exist_ok=True)
    Path("kaggle.json").rename(KAGGLE_TOKEN)
    KAGGLE_TOKEN.chmod(0o600)
print("Kaggle token ready ")

USE_DRIVE = False

if USE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')
    DATA_DIR = Path("/content/drive/MyDrive/celeba")
else:
    DATA_DIR = Path("/content/celeba")

print("Data root:", DATA_DIR)

# Clean and re-create folder
import shutil
if DATA_DIR.exists():
    shutil.rmtree(DATA_DIR)
DATA_DIR.mkdir(parents=True, exist_ok=True)

print(" Downloading CelebA from Kaggle mirror â€¦")
!kaggle datasets download -d jessicali9530/celeba-dataset -p "$DATA_DIR" -q
print("Download complete ")

import zipfile
from pathlib import Path

def unzip_all(root: Path):
    while True:
        zips = list(root.rglob("*.zip"))
        if not zips:
            break
        for z in zips:
            print("ðŸ—œï¸ Unzipping:", z.relative_to(root))
            with zipfile.ZipFile(z, "r") as f:
                f.extractall(z.parent)
            z.unlink()
    print("All zips extracted ")

unzip_all(DATA_DIR)

from pathlib import Path

def find_img_dir(root: Path, min_imgs=1000):
    best = None
    best_n = 0
    for p in root.rglob("*"):
        if p.is_dir():
            n = len(list(p.glob("*.jpg")))
            if n > best_n:
                best, best_n = p, n
    if best is None or best_n < min_imgs:
        raise RuntimeError(f"No JPGs found under {root}. Found {best_n} images total.")
    return best, best_n

IMG_DIR, N_IMGS = find_img_dir(DATA_DIR, min_imgs=1000)
print(" Using image folder:", IMG_DIR)
print(" Total JPGs:", N_IMGS)

from pathlib import Path
from PIL import Image
import glob
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

#  Path to extracted CelebA images
IMG_DIR = Path("/content/celeba/img_align_celeba/img_align_celeba")
assert IMG_DIR.exists(), f" Folder not found: {IMG_DIR}"
print(f" Found folder: {IMG_DIR}")
print(f" Total JPGs: {len(list(IMG_DIR.glob('*.jpg')))}")
# --- Transformations ---
HR_SIZE = 128
SCALE   = 2
LR_SIZE = HR_SIZE // SCALE
hr_tf = transforms.Compose([
    transforms.CenterCrop(178),
    transforms.Resize((HR_SIZE, HR_SIZE), interpolation=Image.BICUBIC),
    transforms.ToTensor(),
])
to_pil = transforms.ToPILImage()
lr_from_hr = transforms.Compose([
    transforms.Resize((LR_SIZE, LR_SIZE), interpolation=Image.BICUBIC),
    transforms.ToTensor(),
])
# --- CelebA class ---
class CelebASR(Dataset):
    def __init__(self, img_dir: Path):
        self.paths = sorted(img_dir.glob("*.jpg"))
        if len(self.paths) == 0:
            raise RuntimeError(f"No images found in {img_dir}")
    def __len__(self): return len(self.paths)
    def __getitem__(self, i):
        pil = Image.open(self.paths[i]).convert("RGB")
        hr  = hr_tf(pil)
        lr  = lr_from_hr(to_pil(hr))
        return lr, hr

sr_ds = CelebASR(IMG_DIR)
print(f" Dataset ready: {len(sr_ds)} images")

from torch.utils.data import DataLoader

sr_loader = DataLoader(sr_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)
print(" DataLoader ready")

from torchvision import utils
import torch
import matplotlib.pyplot as plt
import torch.nn.functional as F

lr_batch, hr_batch = next(iter(sr_loader))

# Make separate grids
grid_lr = utils.make_grid(lr_batch[:8], nrow=8, padding=2)
grid_hr = utils.make_grid(hr_batch[:8], nrow=8, padding=2)

# Upsample LR grid to match HR grid height/width
grid_lr_up = F.interpolate(grid_lr.unsqueeze(0), size=grid_hr.shape[-2:], mode="nearest").squeeze(0)

grid = torch.cat([grid_lr_up, grid_hr], dim=1)

plt.figure(figsize=(10,6))
plt.imshow(grid.permute(1,2,0))
plt.axis("off")
plt.title(f"Top: LR  |  Bottom: HR  ({lr_batch.shape[-1]}â†’{hr_batch.shape[-1]})")
plt.show()

import matplotlib.pyplot as plt
import torch, torch.nn.functional as F

# pick one random sample
i = torch.randint(0, len(sr_ds), (1,)).item()
lr, hr = sr_ds[i]
lr_up = F.interpolate(lr.unsqueeze(0), size=hr.shape[-2:], mode="nearest").squeeze(0)

# choose a patch window
H, W = hr.shape[-2:]
cy, cx, win = H//2, W//2, 40
sl = slice(cy-win, cy+win), slice(cx-win, cx+win)

fig, ax = plt.subplots(1,3, figsize=(12,4))
ax[0].imshow(lr_up.permute(1,2,0));               ax[0].set_title("LR (upsampled)"); ax[0].axis("off")
ax[1].imshow(hr.permute(1,2,0));                  ax[1].set_title("HR");             ax[1].axis("off")
ax[2].imshow(hr[:,sl[0],sl[1]].permute(1,2,0));   ax[2].set_title("HR â€“ zoomed patch"); ax[2].axis("off")
plt.suptitle(f"Sample #{i}  |  LR {lr.shape[-1]}Ã—{lr.shape[-2]} â†’ HR {hr.shape[-1]}Ã—{hr.shape[-2]}")
plt.tight_layout()
plt.show()

print("Total images in dataset:", len(sr_ds))
print("LR batch:", tuple(lr_batch.shape), "HR batch:", tuple(hr_batch.shape))

import torch
from torch.utils.data import Subset

N = len(sr_ds)
g = torch.Generator().manual_seed(42)
perm = torch.randperm(N, generator=g)

test_ratio, val_ratio = 0.05, 0.05
test_size = int(N * test_ratio)
val_size  = int(N * val_ratio)
train_size = N - val_size - test_size

train_idx = perm[:train_size]
val_idx   = perm[train_size:train_size+val_size]
test_idx  = perm[train_size+val_size:]

train_ds = Subset(sr_ds, train_idx.tolist())
val_ds   = Subset(sr_ds, val_idx.tolist())
test_ds  = Subset(sr_ds, test_idx.tolist())

print(f"Split sizes  â–¶  train: {len(train_ds)}  val: {len(val_ds)}  test: {len(test_ds)}")

from google.colab import drive
import os

drive.mount('/content/drive')

out_dir = "/content/drive/MyDrive/celeba/final_tensors_sharded"
os.makedirs(out_dir, exist_ok=True)

print(" Output folder ready:", out_dir)

import os, json
from pathlib import Path
import torch
from torch.utils.data import DataLoader

def to_uint8(x):
    return (x.mul(255).clamp(0,255)).to(torch.uint8)

out_dir = Path("/content/celeba/final_tensors_sharded_fast")
out_dir.mkdir(parents=True, exist_ok=True)

def save_split_tensors_streaming(
    split_ds, name, out_dir,
    batch_size=16,
    shard_size=4096,
    num_workers=0,
    max_items=None,
):
    dl = DataLoader(
        split_ds, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=False, persistent_workers=False
    )

    shard_idx, n_total = 0, 0
    buf_lr, buf_hr = [], []

    for lr, hr in dl:
        buf_lr.append(lr)
        buf_hr.append(hr)
        n_total += lr.size(0)

        cur = sum(x.size(0) for x in buf_lr)
        flush = cur >= shard_size
        if max_items is not None and n_total >= max_items:
            flush = True

        if flush:
            lr_cat = torch.cat(buf_lr, dim=0)
            hr_cat = torch.cat(buf_hr, dim=0)
            if max_items is not None:
                overflow = n_total - max_items
                if overflow > 0:
                    keep = lr_cat.size(0) - overflow
                    lr_cat = lr_cat[:keep]
                    hr_cat = hr_cat[:keep]
                    n_total = max_items

            torch.save(to_uint8(lr_cat), out_dir / f"lr_{name}_{shard_idx:04d}.pt")
            torch.save(to_uint8(hr_cat), out_dir / f"hr_{name}_{shard_idx:04d}.pt")
            shard_idx += 1
            buf_lr, buf_hr = [], []

        if max_items is not None and n_total >= max_items:
            break
    if buf_lr and (max_items is None or n_total < max_items):
        lr_cat = torch.cat(buf_lr, dim=0)
        hr_cat = torch.cat(buf_hr, dim=0)
        torch.save(to_uint8(lr_cat), out_dir / f"lr_{name}_{shard_idx:04d}.pt")
        torch.save(to_uint8(hr_cat), out_dir / f"hr_{name}_{shard_idx:04d}.pt")
        shard_idx += 1

    manifest = {
        "name": name, "count": n_total,
        "batch_size": batch_size, "shard_size": shard_size,
        "num_shards": shard_idx,
        "dtype": "uint8(0-255), RGB, CxHxW",
        "pattern_lr": f"lr_{name}_*.pt",
        "pattern_hr": f"hr_{name}_*.pt",
    }
    with open(out_dir / f"manifest_{name}.json", "w") as f:
        json.dump(manifest, f, indent=2)

    print(f"âœ” Saved {name}: {n_total} items into {manifest['num_shards']} shard(s)")
    return manifest


train_manifest = save_split_tensors_streaming(train_ds, "train", out_dir,
                                              batch_size=16, shard_size=4096,
                                              num_workers=0, max_items=None)
val_manifest   = save_split_tensors_streaming(val_ds, "val", out_dir,
                                              batch_size=16, shard_size=2048,
                                              num_workers=0, max_items=None)
test_manifest  = save_split_tensors_streaming(test_ds, "test", out_dir,
                                              batch_size=16, shard_size=2048,
                                              num_workers=0, max_items=None)

print("All done. Shards at:", out_dir)

import os, math, time, csv, random, shutil
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader


!pip -q install torchmetrics
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

RESULT_DIR = Path('/content/drive/MyDrive/srgan_results')
CKPT_DIR   = RESULT_DIR / 'checkpoints'
SAMPLES_DIR= RESULT_DIR / 'samples'
for d in [RESULT_DIR, CKPT_DIR, SAMPLES_DIR]:
    d.mkdir(parents=True, exist_ok=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)


@torch.no_grad()
def to_image(x):
    """
    x in [0,1] tensor BxCxHxW -> uint8 CPU for saving.
    """
    x = x.clamp(0,1)
    x = (x * 255.0).round().byte()
    return x.cpu()

def save_grid(lr, sr, hr, fname):
    """
    Save a simple horizontal triplet: LRâ†‘, SR, HR
    Assumes lr already upscaled for visualization.
    """
    import torchvision.utils as vutils
    lr_up = F.interpolate(lr, size=hr.shape[-2:], mode='bicubic', align_corners=False)
    grid = torch.cat([lr_up, sr, hr], dim=3)
    vutils.save_image(grid, fname, nrow=1)

def count_parameters(m):
    return sum(p.numel() for p in m.parameters() if p.requires_grad)

#  SRGAN IMPLEMENTATION

import torch, torch.nn as nn, torch.nn.functional as F
from torchvision import datasets, transforms, utils
from torch.utils.data import DataLoader, random_split
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
from torchvision.models import vgg19, VGG19_Weights
import matplotlib.pyplot as plt
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using:", device)

root = "/content/celeba/img_align_celeba/img_align_celeba"
if not os.path.exists(root):
    root = "/content/data"

HR_SIZE = 64
SCALE   = 2
LR_SIZE = HR_SIZE // SCALE

hr_tf = transforms.Compose([
    transforms.Resize((HR_SIZE, HR_SIZE)),
    transforms.ToTensor()
])
lr_tf = transforms.Compose([
    transforms.Resize((LR_SIZE, LR_SIZE)),
    transforms.ToTensor()
])

class SRDataset(torch.utils.data.Dataset):
    def __init__(self, folder):
        self.paths = sorted([os.path.join(folder,f) for f in os.listdir(folder) if f.endswith('.jpg')])[:5000]
    def __len__(self): return len(self.paths)
    def __getitem__(self, i):
        img = transforms.functional.pil_to_tensor(transforms.functional.to_pil_image(plt.imread(self.paths[i])[:,:,:3]))
        hr = hr_tf(transforms.functional.to_pil_image(img))
        lr = lr_tf(transforms.functional.to_pil_image(hr))
        return lr, hr

dataset = SRDataset(root)
n = len(dataset)
train_size = int(0.9*n)
train_ds, val_ds = random_split(dataset, [train_size, n-train_size])
train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)
val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)
print(f"Dataset ready: {len(train_ds)} train / {len(val_ds)} val")

class ResidualBlock(nn.Module):
    def __init__(self, nf=32):
        super().__init__()
        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1)
        self.bn1 = nn.BatchNorm2d(nf)
        self.prelu = nn.PReLU()
        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(nf)
    def forward(self, x):
        out = self.prelu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return x + out

class UpsampleBlock(nn.Module):
    def __init__(self, nf=32, scale=2):
        super().__init__()
        self.conv = nn.Conv2d(nf, nf*(scale**2), 3, 1, 1)
        self.ps = nn.PixelShuffle(scale)
        self.prelu = nn.PReLU()
    def forward(self, x):
        return self.prelu(self.ps(self.conv(x)))

class Generator(nn.Module):
    def __init__(self, in_ch=3, nf=32, nb=4, scale=2):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, nf, 9, 1, 4)
        self.prelu = nn.PReLU()
        self.res = nn.Sequential(*[ResidualBlock(nf) for _ in range(nb)])
        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(nf)
        self.up = UpsampleBlock(nf, scale)
        self.conv3 = nn.Conv2d(nf, in_ch, 9, 1, 4)
    def forward(self, x):
        x1 = self.prelu(self.conv1(x))
        x2 = self.res(x1)
        x2 = self.bn2(self.conv2(x2))
        x = x1 + x2
        x = self.up(x)
        return torch.sigmoid(self.conv3(x))

class Discriminator(nn.Module):
    def __init__(self, in_ch=3):
        super().__init__()
        nf = 32
        self.main = nn.Sequential(
            nn.Conv2d(in_ch, nf, 3, 1, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(nf, nf*2, 3, 2, 1),
            nn.BatchNorm2d(nf*2), nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(nf*2, nf*4, 3, 2, 1),
            nn.BatchNorm2d(nf*4), nn.LeakyReLU(0.2, inplace=True),
            nn.Flatten(),
            nn.Linear((HR_SIZE//4)*(HR_SIZE//4)*nf*4, 1)
        )
    def forward(self, x):
        return self.main(x)

class VGGFeature(nn.Module):
    def __init__(self):
        super().__init__()
        vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features[:17].eval()
        for p in vgg.parameters(): p.requires_grad=False
        self.vgg=vgg
    def forward(self,x): return self.vgg(x)

G = Generator().to(device)
D = Discriminator().to(device)
vgg = VGGFeature().to(device)
optG = torch.optim.Adam(G.parameters(), lr=2e-4)
optD = torch.optim.Adam(D.parameters(), lr=2e-4)
bce = nn.BCEWithLogitsLoss()

for epoch in range(3):
    G.train(); D.train()
    for lr, hr in train_loader:
        lr, hr = lr.to(device), hr.to(device)

        with torch.no_grad(): sr = G(lr)
        d_real = D(hr); d_fake = D(sr)
        loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))
        optD.zero_grad(); loss_d.backward(); optD.step()

        # Train G
        sr = G(lr)
        adv = bce(D(sr), torch.ones_like(d_fake))
        perc = F.l1_loss(vgg(sr), vgg(hr))
        pix = F.l1_loss(sr, hr)
        loss_g = pix + 0.1*perc + 1e-3*adv
        optG.zero_grad(); loss_g.backward(); optG.step()
    print(f"Epoch {epoch+1}: G={loss_g.item():.4f}, D={loss_d.item():.4f}")

# EVALUATION

@torch.no_grad()
def evaluate(G, loader):
    psnr = PeakSignalNoiseRatio(data_range=1.0).to(device)
    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)
    G.eval()
    p, s, n = 0, 0, 0
    for lr, hr in loader:
        lr, hr = lr.to(device), hr.to(device)
        sr = G(lr)
        p += psnr(sr, hr).item()
        s += ssim(sr, hr).item()
        n += 1
    return p/n, s/n

mean_psnr, mean_ssim = evaluate(G, val_loader)
print(f"\nâœ… Mean PSNR: {mean_psnr:.2f} dB")
print(f"âœ… Mean SSIM: {mean_ssim:.4f}")


lr, hr = next(iter(val_loader))
with torch.no_grad(): sr = G(lr.to(device)).cpu()
utils.save_image(torch.cat([F.interpolate(lr, size=hr.shape[-2:], mode="bicubic"), sr, hr], dim=0),
                 "srgan_results.png", nrow=3)
plt.imshow(plt.imread("srgan_results.png"))
plt.axis("off")
plt.show()

import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    def __init__(self, nf=32):
        super().__init__()
        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1)
        self.bn1 = nn.BatchNorm2d(nf)
        self.prelu = nn.PReLU()
        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(nf)
    def forward(self, x):
        out = self.prelu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return x + out

class UpsampleBlock(nn.Module):
    def __init__(self, nf=32, scale=2):
        super().__init__()
        self.conv = nn.Conv2d(nf, nf * (scale ** 2), 3, 1, 1)
        self.ps = nn.PixelShuffle(scale)
        self.prelu = nn.PReLU()
    def forward(self, x):
        return self.prelu(self.ps(self.conv(x)))

class Generator(nn.Module):
    def __init__(self, in_ch=3, nf=32, nb=4, scale=2):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, nf, 9, 1, 4)
        self.prelu = nn.PReLU()
        self.res = nn.Sequential(*[ResidualBlock(nf) for _ in range(nb)])
        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(nf)
        self.up = UpsampleBlock(nf, scale)
        self.conv3 = nn.Conv2d(nf, in_ch, 9, 1, 4)
    def forward(self, x):
        x1 = self.prelu(self.conv1(x))
        x2 = self.res(x1)
        x2 = self.bn2(self.conv2(x2))
        x = x1 + x2
        x = self.up(x)
        return torch.sigmoid(self.conv3(x))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
G = Generator().to(device)

import os, csv, math, numpy as np
from pathlib import Path
import torch
import torch.nn.functional as F
from torchvision.utils import save_image
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
.
try:
    from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
    _lpips_ok = True
except Exception:
    _lpips_ok = False
try:
    from google.colab import drive
    drive.mount('/content/drive', force_remount=False)
    DRIVE_ROOT = "/content/drive/MyDrive"
except Exception:
    DRIVE_ROOT = os.path.expanduser("~/MyDrive")

RESULT_DIR = Path(DRIVE_ROOT) / "srgan_results"
CKPT_DIR   = RESULT_DIR / "checkpoints"
VIZ_DIR    = RESULT_DIR / "viz"
for d in (CKPT_DIR, VIZ_DIR):
    d.mkdir(parents=True, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def save_checkpoint(path, G, D=None, optG=None, optD=None, **extra):
    """
    Save Generator + (optional) Discriminator and optimizers to a single .pt.
    """
    payload = {"G": G.state_dict()}
    if D is not None:    payload["D"]    = D.state_dict()
    if optG is not None: payload["optG"] = optG.state_dict()
    if optD is not None: payload["optD"] = optD.state_dict()
    payload.update(extra)
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(payload, path)
    print(f"âœ… Saved checkpoint â†’ {path}")

def load_checkpoint(path, G=None, D=None, optG=None, optD=None, map_location=None):
    """
    Load previously saved states into provided modules/optimizers (if passed).
    Returns the checkpoint dict (e.g., to read epoch/metrics you saved).
    """
    chk = torch.load(path, map_location=map_location or device)
    if G is not None and "G" in chk:
        G.load_state_dict(chk["G"]); G.to(device).eval()
        print("  â€¢ loaded G")
    if D is not None and "D" in chk:
        D.load_state_dict(chk["D"]); D.to(device).eval()
        print("  â€¢ loaded D")
    if optG is not None and "optG" in chk:
        optG.load_state_dict(chk["optG"]); print("  â€¢ loaded optG")
    if optD is not None and "optD" in chk:
        optD.load_state_dict(chk["optD"]); print("  â€¢ loaded optD")
    print(f"âœ… Loaded checkpoint â† {path}")
    return chk

@torch.no_grad()
def evaluate_metrics(G, loader, csv_path=None, max_batches=None):
    """
    Evaluate G on (lr, hr) loader. Returns a dict of mean metrics.
    If csv_path is set, writes per-batch metrics there.
    """
    G.eval().to(device)
    psnr_m = PeakSignalNoiseRatio(data_range=1.0).to(device)
    ssim_m = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)
    lpips_m = LearnedPerceptualImagePatchSimilarity(net_type="vgg").to(device) if _lpips_ok else None

    sums = {"psnr":0.0, "ssim":0.0, "mse":0.0, "rmse":0.0, "mae":0.0}
    if lpips_m is not None: sums["lpips"] = 0.0
    rows = []
    n = 0

    for bi, (lr, hr) in enumerate(loader):
        lr, hr = lr.to(device), hr.to(device)
        sr = G(lr).clamp(0,1)

        psnr = psnr_m(sr, hr).item()
        ssim = ssim_m(sr, hr).item()
        mse  = F.mse_loss(sr, hr).item()
        rmse = math.sqrt(mse)
        mae  = F.l1_loss(sr, hr).item()
        if lpips_m is not None:
            lpips = lpips_m(sr, hr).item()
        else:
            lpips = None

        sums["psnr"] += psnr; sums["ssim"] += ssim
        sums["mse"]  += mse;  sums["rmse"] += rmse; sums["mae"] += mae
        if lpips is not None: sums["lpips"] += lpips
        n += 1

        row = [bi, psnr, ssim, mse, rmse, mae]
        if lpips is not None: row.append(lpips)
        rows.append(row)

        if max_batches and (bi+1) >= max_batches: break

    means = {k: v/max(n,1) for k,v in sums.items()}
    print("ðŸ“Š Means â†’ PSNR: %.2f dB | SSIM: %.4f | MSE: %.6f | RMSE: %.5f | MAE: %.5f%s" %
          (means["psnr"], means["ssim"], means["mse"], means["rmse"], means["mae"],
           f" | LPIPS: {means['lpips']:.4f}" if "lpips" in means else ""))

    if csv_path is not None:
        header = ["batch_idx","psnr","ssim","mse","rmse","mae"] + (["lpips"] if "lpips" in means else [])
        with open(csv_path, "w", newline="") as f:
            w = csv.writer(f); w.writerow(header); w.writerows(rows)
        print(f"âœ… Wrote per-batch metrics CSV â†’ {csv_path}")
    return means


@torch.no_grad()
def make_visuals(G, loader, out_dir=VIZ_DIR, rows=8, zoom=64, pool_batches=80):
    """
    Saves:
      - grid_random.png            (columns = LRâ†‘ | SRGAN | HR)
      - grid_best_psnr.png         (top PSNR batches)
      - grid_worst_psnr.png        (lowest PSNR batches)
      - grid_zoom_<zoom>.png       (center crops)
      - grid_error_heatmap.png     (|SR-HR| normalized)
    """
    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)
    G.eval().to(device)
    psnr_m = PeakSignalNoiseRatio(data_range=1.0).to(device)

    pool = []
    for bi, (lr, hr) in enumerate(loader):
        lr, hr = lr.to(device), hr.to(device)
        sr = G(lr).clamp(0,1)
        p = psnr_m(sr, hr).item()
        pool.append((lr.cpu(), sr.cpu(), hr.cpu(), p))
        if (bi+1) >= pool_batches: break

    def save_triplet_grid(items, path, nrow=rows):
        l = torch.cat([t[0] for t in items], 0)
        s = torch.cat([t[1] for t in items], 0)
        h = torch.cat([t[2] for t in items], 0)
        l_up = F.interpolate(l, size=h.shape[-2:], mode="bicubic", align_corners=False)
        grid = torch.cat([l_up, s, h], 0)
        save_image(grid, str(path), nrow=nrow)


    chunk, taken = [], 0
    for (lr, sr, hr, _) in pool:
        b = min(lr.size(0), rows - taken)
        if b <= 0: break
        chunk.append((lr[:b], sr[:b], hr[:b])); taken += b
    save_triplet_grid(chunk, Path(out_dir, "grid_random.png"))
    print("ðŸ–¼ï¸", Path(out_dir, "grid_random.png"))

    best  = sorted(pool, key=lambda x: x[3], reverse=True)[:max(1, rows//2)]
    worst = sorted(pool, key=lambda x: x[3])[:max(1, rows//2)]
    save_triplet_grid(best,  Path(out_dir, "grid_best_psnr.png"),
                      nrow=min(rows, sum(t[0].size(0) for t in best)))
    save_triplet_grid(worst, Path(out_dir, "grid_worst_psnr.png"),
                      nrow=min(rows, sum(t[0].size(0) for t in worst)))
    print("ðŸ–¼ï¸", Path(out_dir, "grid_best_psnr.png"), "|", Path(out_dir, "grid_worst_psnr.png"))

    h = torch.cat([t[2] for t in chunk], 0)
    s = torch.cat([t[1] for t in chunk], 0)
    l = torch.cat([t[0] for t in chunk], 0)
    l_up = F.interpolate(l, size=h.shape[-2:], mode="bicubic", align_corners=False)
    H, W = h.shape[-2:]; cy, cx = H//2, W//2; hs = zoom//2
    def crop(x): return x[..., cy-hs:cy+hs, cx-hs:cx+hs]
    zoom_grid = torch.cat([crop(l_up), crop(s), crop(h)], 0)
    save_image(zoom_grid, str(Path(out_dir, f"grid_zoom_{zoom}.png")), nrow=rows)
    print("ðŸ–¼ï¸", Path(out_dir, f"grid_zoom_{zoom}.png"))

    err = (s - h).abs()
    emax = err.view(err.size(0), -1).max(1)[0].clamp_min(1e-8).view(-1,1,1,1)
    err = err / emax
    save_image(err, str(Path(out_dir, "grid_error_heatmap.png")), nrow=rows)
    print("ðŸ–¼ï¸", Path(out_dir, "grid_error_heatmap.png"))

save_checkpoint(CKPT_DIR / "srgan_celeba_best.pt", G, D, optG=None, optD=None, epoch="final")

evaluate_metrics(G, val_loader, csv_path=RESULT_DIR / "val_metrics.csv")
make_visuals(G, val_loader, out_dir=VIZ_DIR, rows=8, zoom=64, pool_batches=100)

